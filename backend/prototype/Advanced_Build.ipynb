{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ðŸ¤ Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "- ðŸ¤ Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ðŸ¤ Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "3fa78560-393c-4ee5-b871-9886bf0d70f4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkla2fpx28QK",
        "outputId": "52d7ad22-fcb1-4abe-853b-216c55a12650"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b69df90a-b4e1-4ddb-9de0-882d98b68ab2"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE7 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain-community/tree/main/libs/community) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Tavily Search Results](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/tavily_search/tool.py)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/arxiv/tool.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")\n",
        "\n",
        "thtc_docs = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "208"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(thtc_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    documents=thtc_docs,\n",
        "    embedding=embedding_model,\n",
        "    location=\":memory:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from typing import Annotated, List, Tuple, Union\n",
        "@tool\n",
        "def retrieve(\n",
        "    query: Annotated[str, \"query about information in the tight hips, twisted core book\"]\n",
        "    ):\n",
        "  \"\"\"provides detailed information about the book \"Tight Hip, Twisted Core\" by Christine Koth, which focuses on understanding, diagnosing, treating, and preventing issues related to tight hip flexors, particularly the iliacus muscle, and their widespread impact on the body. It can help agents answer queries about the causes and effects of iliopsoas tightness throughout the body, as well as solutions for relief and prevention, including self-treatment and professional interventions\"\"\"\n",
        "  retrieved_docs = qdrant_retriever.invoke(query)\n",
        "  return {\"context\" : retrieved_docs}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/r5/wsbp5_jj7lsfh6rxrw7p7lv00000gn/T/ipykernel_46537/76110118.py:4: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
            "  tavily_tool = TavilySearchResults(max_results=5)\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=5)\n",
        "\n",
        "from langchain_core.tools import Tool\n",
        "\n",
        "tool_belt = [\n",
        "    tavily_tool,\n",
        "    ArxivQueryRun(),\n",
        "    Tool.from_function(\n",
        "        func=retrieve,\n",
        "        name=\"Retrieve\",\n",
        "        description=\"provides detailed information about the book `Tight Hip, Twisted Core` by Christine Koth, which focuses on understanding, diagnosing, treating, and preventing issues related to tight hip flexors, particularly the iliacus muscle, and their widespread impact on the body. It can help agents answer queries about the causes and effects of iliopsoas tightness throughout the body, as well as solutions for relief and prevention, including self-treatment and professional interventions\")\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tells model how it can call the functions. provides a json describing each tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "#### âœ… ANSWER\n",
        "When we bind tools to the model, it establishes a metadata field called TOOLS that looks like this:\n",
        "\n",
        "```\n",
        "tools: [\n",
        "    {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"tavily_search_results_json\",\n",
        "        \"description\": \"A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\",\n",
        "        \"parameters\": {\n",
        "        \"properties\": {\n",
        "            \"query\": {\n",
        "            \"description\": \"search query to look up\",\n",
        "            \"type\": \"string\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\n",
        "            \"query\"\n",
        "        ],\n",
        "        \"type\": \"object\"\n",
        "        }\n",
        "    }\n",
        "    }, {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"arxiv\",\n",
        "        \"description\": \"A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\",\n",
        "        \"parameters\": {\n",
        "        \"properties\": {\n",
        "            \"query\": {\n",
        "            \"description\": \"search query to look up\",\n",
        "            \"type\": \"string\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\n",
        "            \"query\"\n",
        "        ],\n",
        "        \"type\": \"object\"\n",
        "        }\n",
        "    }\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "The model reads the description of each tool to determine which one works best for the task it is trying to complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "# use context from previous parts of agent to achieve it's task\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def planner_node(state):\n",
        "    query = state[\"messages\"][-1].content\n",
        "    plan = model.invoke(f\"Break down the question into research steps: {query}\")\n",
        "    return {\"messages\": [plan]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "# we call the tool with the payload provided by the model\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vF4_lgtmQNo",
        "outputId": "a4384377-8f7a-415f-be1b-fee6169cb101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x13316cec0>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "# uncompiled_graph.add_node(\"planner\", planner_node)\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGCbaYqRnmiw",
        "outputId": "5351807c-2ac7-4316-a3a3-878abeacd114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x13316cec0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")\n",
        "# uncompiled_graph.add_edge(\"planner\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BZgb81VQf9o",
        "outputId": "73a07c15-5f0b-40f2-b033-38b57d056dd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x13316cec0>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcgbHf1rIXZ",
        "outputId": "45d4bdd6-d6bb-4a1d-bb79-cad43c130bf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x13316cec0>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "simple_agent_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x13316d010>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_agent_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### âœ… ANSWER\n",
        "\n",
        "**Is there a specific limit?**\n",
        "- No, LangGraph does not impose a default limit on cycles\n",
        "- Infinite loops are possible and can cause memory/resource exhaustion\n",
        "\n",
        "**How to implement limits:**\n",
        "\n",
        "1. **Message count limit** (as shown in our code):\n",
        "   ```python\n",
        "   if len(state[\"messages\"]) > 10:\n",
        "       return \"END\"\n",
        "   ```\n",
        "\n",
        "2. **Recursion limit at compile time**:\n",
        "   ```python\n",
        "   graph.compile(recursion_limit=20)\n",
        "   ```\n",
        "\n",
        "3. **Custom state counter**:\n",
        "   ```python\n",
        "   class AgentState(TypedDict):\n",
        "       messages: Annotated[list, add_messages]\n",
        "       cycle_count: int\n",
        "   ```\n",
        "\n",
        "4. **Implementation locations**:\n",
        "   - Conditional edges (as shown in our example)\n",
        "   - Individual nodes\n",
        "   - At graph compilation time\n",
        "\n",
        "The notebook demonstrates the message count approach, which is simple and effective for most use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "5eeedfae-089d-496e-e71f-071939fa5832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_lno0YSC6FmL0R428rbPc7SSI', 'function': {'arguments': '{\"query\": \"iliacus muscle function and impact on the body\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_I5vQ8JfiFSrIhweUFrn5qkiV', 'function': {'arguments': '{\"query\": \"workout plan for iliacus muscle\"}', 'name': 'arxiv'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 292, 'total_tokens': 350, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BuNdjwp12vFfsXKzBBKDlHeGZDiDT', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--3d23d445-3c01-48e3-88f4-72cd5b1a9f2c-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'iliacus muscle function and impact on the body'}, 'id': 'call_lno0YSC6FmL0R428rbPc7SSI', 'type': 'tool_call'}, {'name': 'arxiv', 'args': {'query': 'workout plan for iliacus muscle'}, 'id': 'call_I5vQ8JfiFSrIhweUFrn5qkiV', 'type': 'tool_call'}], usage_metadata={'input_tokens': 292, 'output_tokens': 58, 'total_tokens': 350, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content=\"Published: 2023-12-08\\nTitle: Insomnia impairs muscle function via regulating protein degradation and muscle clock\\nAuthors: Hui Ouyang, Hong Jiang, Jin Huang, Zunjing Liu\\nSummary: Background: Insomnia makes people more physically unable of doing daily\\nduties, which results in a lack of strength, leads to lacking in strength.\\nHowever, the effects of insomnia on muscle function have not yet been\\nthoroughly investigated. So, the objectives of this study were to clarify how\\ninsomnia contributes to the decrease of muscular function and to investigate\\nthe mechanisms behind this phenomenon. Methods: To understand how insomnia\\ninfluence muscle function, we analyzed the expression level of factors\\nassociated with muscle protein degradation, muscle protein synthesis , protein\\nsynthesis and degradation pathways and muscle clock. Results: The results\\nshowed that lower BMI and grip strength were observed in insomnia patients. The\\nmice in the sleep deprivation(SD) group saw a 7.01 g loss in body mass. The SD\\ngroup's tibialis anterior and gastrocnemius muscle mass decreased after 96 h of\\nSD). The grip strength reduced in SD group. Using the RT-PCR approaches, we\\nfound a significant increase in muscle degradation factors expression in SD\\ngroup versus normal control group. Conclusions: Insomnia can impair muscle\\nfunction. The mechanism may be associated with the increased expression of\\nmuscle degradation related factors , as well as the abnormal expression of\\nClock gene.\\n\\nPublished: 2020-08-14\\nTitle: Large-Scale Analysis of Iliopsoas Muscle Volumes in the UK Biobank\\nAuthors: Julie Fitzpatrick, Nicolas Basty, Madeleine Cule, Yi Liu, Jimmy D. Bell, E. Louise Thomas, Brandon Whitcher\\nSummary: Psoas muscle measurements are frequently used as markers of sarcopenia and\\npredictors of health. Manually measured cross-sectional areas are most commonly\\nused, but there is a lack of consistency regarding the position of the\\nmeasurementand manual annotations are not practical for large population\\nstudies. We have developed a fully automated method to measure iliopsoas muscle\\nvolume (comprised of the psoas and iliacus muscles) using a convolutional\\nneural network. Magnetic resonance images were obtained from the UK Biobank for\\n5,000 male and female participants, balanced for age, gender and BMI. Ninety\\nmanual annotations were available for model training and validation. The model\\nshowed excellent performance against out-of-sample data (dice score coefficient\\nof 0.912 +/- 0.018). Iliopsoas muscle volumes were successfully measured in all\\n5,000 participants. Iliopsoas volume was greater in male compared with female\\nsubjects. There was a small but significant asymmetry between left and right\\niliopsoas muscle volumes. We also found that iliopsoas volume was significantly\\nrelated to height, BMI and age, and that there was an acceleration in muscle\\nvolume decrease in men with age. Our method provides a robust technique for\\nmeasuring iliopsoas muscle volume that can be applied to large cohorts.\\n\\nPublished: 2024-10-10\\nTitle: Patterned Structure Muscle : Arbitrary Shaped Wire-driven Artificial Muscle Utilizing Anisotropic Flexible Structure for Musculoskeletal Robots\\nAuthors: Shunnosuke Yoshimura, Akihiro Miki, Kazuhiro Miyama, Yuta Sahara, Kento Kawaharazuka, Kei Okada, Masayuki Inaba\\nSummary: Muscles of the human body are composed of tiny actuators made up of myosin\\nand actin filaments. They can exert force in various shapes such as curved or\\nflat, under contact forces and deformations from the environment. On the other\\nhand, muscles in musculoskeletal robots so far have faced challenges in\\ngenerating force in such shapes and environments. To address this issue, we\\npropose Patterned Structure Muscle (PSM), artificial muscles for\\nmusculoskeletal robots. PSM utilizes patterned structures with anisotropic\\ncharacteristics, wire-driven mechanisms, and is made of flexible material\\nThermoplastic Polyurethane (TPU) using FDM 3D printing. This method enables the\\ncreation of \", name='arxiv', id='ecf49b8d-1dae-4915-9b84-6485fd2dc973', tool_call_id='call_lno0YSC6FmL0R428rbPc7SSI'), ToolMessage(content=\"Published: 2020-08-14\\nTitle: Large-Scale Analysis of Iliopsoas Muscle Volumes in the UK Biobank\\nAuthors: Julie Fitzpatrick, Nicolas Basty, Madeleine Cule, Yi Liu, Jimmy D. Bell, E. Louise Thomas, Brandon Whitcher\\nSummary: Psoas muscle measurements are frequently used as markers of sarcopenia and\\npredictors of health. Manually measured cross-sectional areas are most commonly\\nused, but there is a lack of consistency regarding the position of the\\nmeasurementand manual annotations are not practical for large population\\nstudies. We have developed a fully automated method to measure iliopsoas muscle\\nvolume (comprised of the psoas and iliacus muscles) using a convolutional\\nneural network. Magnetic resonance images were obtained from the UK Biobank for\\n5,000 male and female participants, balanced for age, gender and BMI. Ninety\\nmanual annotations were available for model training and validation. The model\\nshowed excellent performance against out-of-sample data (dice score coefficient\\nof 0.912 +/- 0.018). Iliopsoas muscle volumes were successfully measured in all\\n5,000 participants. Iliopsoas volume was greater in male compared with female\\nsubjects. There was a small but significant asymmetry between left and right\\niliopsoas muscle volumes. We also found that iliopsoas volume was significantly\\nrelated to height, BMI and age, and that there was an acceleration in muscle\\nvolume decrease in men with age. Our method provides a robust technique for\\nmeasuring iliopsoas muscle volume that can be applied to large cohorts.\\n\\nPublished: 2022-10-21\\nTitle: Domain Knowledge-Informed Self-Supervised Representations for Workout Form Assessment\\nAuthors: Paritosh Parmar, Amol Gharat, Helge Rhodin\\nSummary: Maintaining proper form while exercising is important for preventing injuries\\nand maximizing muscle mass gains. Detecting errors in workout form naturally\\nrequires estimating human's body pose. However, off-the-shelf pose estimators\\nstruggle to perform well on the videos recorded in gym scenarios due to factors\\nsuch as camera angles, occlusion from gym equipment, illumination, and\\nclothing. To aggravate the problem, the errors to be detected in the workouts\\nare very subtle. To that end, we propose to learn exercise-oriented image and\\nvideo representations from unlabeled samples such that a small dataset\\nannotated by experts suffices for supervised error detection. In particular,\\nour domain knowledge-informed self-supervised approaches (pose contrastive\\nlearning and motion disentangling) exploit the harmonic motion of the exercise\\nactions, and capitalize on the large variances in camera angles, clothes, and\\nillumination to learn powerful representations. To facilitate our\\nself-supervised pretraining, and supervised finetuning, we curated a new\\nexercise dataset, \\\\emph{Fitness-AQA}\\n(\\\\url{https://github.com/ParitoshParmar/Fitness-AQA}), comprising of three\\nexercises: BackSquat, BarbellRow, and OverheadPress. It has been annotated by\\nexpert trainers for multiple crucial and typically occurring exercise errors.\\nExperimental results show that our self-supervised representations outperform\\noff-the-shelf 2D- and 3D-pose estimators and several other baselines. We also\\nshow that our approaches can be applied to other domains/tasks such as pose\\nestimation and dive quality assessment.\\n\\nPublished: 2024-06-10\\nTitle: Video-based Exercise Classification and Activated Muscle Group Prediction with Hybrid X3D-SlowFast Network\\nAuthors: Manvik Pasula, Pramit Saha\\nSummary: This paper introduces a simple yet effective strategy for exercise\\nclassification and muscle group activation prediction (MGAP). These tasks have\\nsignificant implications for personal fitness, facilitating more affordable,\\naccessible, safer, and simpler exercise routines. This is particularly relevant\\nfor novices and individuals with disabilities. Previous research in the field\\nis mostly dominated by the reliance on mounted sensors and a limited scope of\\nexercises, reducing practicality for everyday use. F\", name='arxiv', id='46beeec4-f869-4690-a6bf-6a3c5d539bf0', tool_call_id='call_I5vQ8JfiFSrIhweUFrn5qkiV')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"The research on the iliacus muscle indicates that it plays a significant role in the body's musculoskeletal and functional health. The iliacus, along with the psoas major, forms the iliopsoas group, which is crucial for hip flexion, stabilization, and movement. Studies have shown that the volume and health of the iliopsoas muscle are related to overall physical health, age, and activity levels. For example, decreased iliopsoas volume is associated with aging and sarcopenia, and maintaining its strength can be important for mobility and preventing injuries.\\n\\nRegarding the impact on the body, a tight or weak iliacus can contribute to various issues such as lower back pain, hip dysfunction, and altered gait patterns. Insomnia and other factors that impair muscle function can lead to increased muscle degradation, which may affect the iliacus among other muscles, impacting overall mobility and stability.\\n\\nAs for workout plans, targeted exercises can help strengthen and maintain the flexibility of the iliacus muscle. Here is a suggested workout plan:\\n\\n### Workout Plan to Strengthen and Flexibilize the Iliacus Muscle\\n\\n**Warm-up (5-10 minutes)**\\n- Light cardio (walking, cycling)\\n- Dynamic stretches (leg swings, hip circles)\\n\\n**Main Exercises**\\n1. **Hip Flexor Stretch**\\n   - Kneel on one knee, other foot in front, and gently push hips forward.\\n   - Hold for 30 seconds each side.\\n2. **Leg Raises**\\n   - Lie on your back, lift one leg straight up while keeping the other on the ground.\\n   - Perform 3 sets of 10 reps per leg.\\n3. **Standing Hip Flexion**\\n   - Stand upright, lift one knee towards your chest, hold for a second, then lower.\\n   - Perform 3 sets of 12 reps per leg.\\n4. **Lunge with Hip Flexor Stretch**\\n   - Step forward into a lunge position, gently push hips downward.\\n   - Hold for 30 seconds each side.\\n5. **Bridge with Knee Drive**\\n   - Lie on your back, lift hips, then bring one knee towards your chest.\\n   - Perform 3 sets of 12 reps per leg.\\n6. **Core Engagement Exercises**\\n   - Planks, side planks, and leg lifts to support hip stability.\\n\\n**Cool-down (5 minutes)**\\n- Gentle stretching focusing on hip flexors and hamstrings.\\n\\n**Additional Tips**\\n- Maintain proper posture during exercises.\\n- Incorporate flexibility training regularly.\\n- Consult a physical therapist for personalized guidance, especially if experiencing pain or dysfunction.\\n\\nWould you like more detailed instructions or variations for these exercises?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 537, 'prompt_tokens': 2120, 'total_tokens': 2657, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BuNdmTGYB7AzUElAsvJ08QVfk2vDa', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1ba50e06-9226-4f21-9e46-650f7ab8a386-0', usage_metadata={'input_tokens': 2120, 'output_tokens': 537, 'total_tokens': 2657, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"how does the iliacus muscle affect the body and what are some research papers on it? Design a workout plan to help with the iliacus muscle\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x10d7fa900>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x10d7fa900>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_agent_values(values):\n",
        "    for key, item in values.items():\n",
        "            if key ==\"messages\":\n",
        "                print(\"Messages:\")\n",
        "                for msg in item:\n",
        "                    print(f\"  Type: {type(msg).__name__}\")\n",
        "                    if hasattr(msg, \"content\"):\n",
        "                        print(f\"  Content: {msg.content}\")\n",
        "                    if hasattr(msg, \"function_call\"):\n",
        "                        print(f\"  Function Call: {msg.function_call}\")\n",
        "            else:\n",
        "                print(f\"{key}: {item}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "ff009536-d281-4a56-c126-9cd245352bfe"
      },
      "outputs": [],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using Tavily!\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print_agent_values(values)\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, I don't think the answer is correct. It identifies the authors a but does not show their latest tweets. It shows a summary of some tweets but it is not clear that they are from the authors.\n",
        "I used LangSmith to understand the steps taken\n",
        "\n",
        "Steps:\n",
        "1. Visit the `agent` node and provide the query to the LLM. The LLM requests:\n",
        "    - tool call to arxiv to retrieve research papers related to QLora\n",
        "    - 3 tool calls to tavily to fetch the latest tweets for each author\n",
        "2. Visit the `should_continue` edge. Because the last message includes tool calls it returns \"action\"\n",
        "3. Visit the `action` node\n",
        "    - 1 call to arxiv which retreives a paper on QLoRa. The contents of the paper include the authors\n",
        "    - 3 calls to tavily which retrieve some tweets in addition to some medium articles, github posts and other web content\n",
        "        - None specifically include tweets from the authors\n",
        "5. Visit the agent, with the results of the tool call in context\n",
        "    - Make an LLM request but now with the tool call info in the context window\n",
        "    - returns an answer\n",
        "6. Visit the `should_continue` edge. The LLM decides the answer is accurate so \"action\" is returned\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¤ Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain_with_formatting = convert_inputs | simple_agent_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "76be837b-6424-4516-8f63-07fbd8c25bf5"
      },
      "outputs": [],
      "source": [
        "agent_chain_with_formatting.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What are the key components of a LangGraph system?\",\n",
        "    \"How does LangGraph differ from a regular LangChain agent?\",\n",
        "    \"What is the purpose of the StateGraph class in LangGraph?\",\n",
        "    \"How do you handle branching logic in LangGraph?\",\n",
        "    \"What are the benefits of using LangGraph over traditional agents?\",\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\": [\"nodes\", \"edges\", \"state\"]},\n",
        "    {\"must_mention\": [\"graph\", \"workflow\", \"state management\"]}, \n",
        "    {\"must_mention\": [\"state\", \"transitions\", \"graph\"]},\n",
        "    {\"must_mention\": [\"conditional\", \"routing\", \"edges\"]},\n",
        "    {\"must_mention\": [\"complex workflows\", \"state tracking\", \"debugging\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### âœ… ANSWER\n",
        "\n",
        "An answer is considered correct if it contains all of the keywords mentioned in the `must_mention` field.\n",
        "\n",
        "**Why this approach is problematic**\n",
        "This evaluation method has significant limitations:\n",
        "1. Keyword presence â‰  Correctness: Simply including required keywords doesn't guarantee the answer is factually accurate or logically sound.\n",
        "2. Hallucination risk: The LLM could generate responses that contain all the required keywords but provide completely incorrect explanations or context around them.\n",
        "3. Incomplete evaluation: This approach only validates partial content rather than overall response quality, meaning it misses:\n",
        "- Logical consistency\n",
        "- Factual accuracy\n",
        "- Proper context and explanation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "â“ Question #4:\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "### âœ… ANSWER\n",
        "Recommended improvements:\n",
        "To create a more robust evaluation system, consider implementing:\n",
        "- Semantic evaluation: Use LLM-based evaluators to assess the overall correctness of explanations, not just keyword presence\n",
        "- Reference answer comparison: Provide gold standard answers for more comprehensive evaluation\n",
        "- Multi-criteria assessment: Combine keyword checking with accuracy, relevance, and completeness metrics\n",
        "- Context-aware evaluation: Ensure keywords are used correctly within proper explanations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "efcf57067cf743d8b4ce059a61cbe02e",
            "53e33aae3b97490c82aec7bbb0d6ebba",
            "ad84e0e971d3455db2efe7dd0d1f803e",
            "72adef9b70dd48198b7322b6c5b113cf",
            "8a61d045ffd44ac58f3f13eb10044836",
            "041e22a9b5514e36bd4d1dac01d5d398",
            "886d762f2a7c421382efb5502c6d42a1",
            "ab91fd625bbd43afbf8c6398193a88d0",
            "716557ad09874dcb989d75f7c74424cd",
            "77d4c0ebaae045b58efc4f789c9a2360",
            "0d622ccc56264fac8fd7508dbdbe6e29"
          ]
        },
        "id": "p5TeCUUkuGld",
        "outputId": "2f7d62a2-e78d-447a-d07b-f9e4d500fb79"
      },
      "outputs": [],
      "source": [
        "experiment_results = client.evaluate(\n",
        "    agent_chain_with_formatting,\n",
        "    data=dataset_name,\n",
        "    evaluators=[must_mention],\n",
        "    experiment_prefix=f\"Search Pipeline - Evaluation - {uuid4().hex[0:4]}\",\n",
        "    metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "eeEqU7s05Byu",
        "outputId": "78395075-a05d-4ebd-c798-ed968b935318"
      },
      "outputs": [],
      "source": [
        "experiment_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### CREATING A NEW GRAPH AND ADDING AGENT AND ACTION NODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r6XXA5FJbVf",
        "outputId": "ff713041-e498-4f0f-a875-a03502b87729"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### MAKE THE AGENT NODE THE FIRST NODE IN THE GRAPH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNWHwWxuRiLY",
        "outputId": "295f5a35-ceff-452a-ffb8-c52eada6a816"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### CREATING A CONDITIONAL NODE TO DETERMINE WHETHER TO CONTINUE OR END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  helpfullness_prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "\n",
        "  helpfulness_chain = helpfullness_prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!\n",
        "\n",
        "### âœ… ANSWER\n",
        "\n",
        "1. Check the last emitted message. If it contains a tool call this indicates that the LLM is requesting some tool action so return \"action\"\n",
        "2. If there is no request for a tool call, we check to see how many message have accumulated in our agent. If it exceeds 10, we return \"END\" indicating that we should stop the agent reasoning loop\n",
        "3. If not we create a chain and use it to ask the LLM if the latest response is helpful in answering the users query\n",
        "4. If the LLM deems it helpful, return \"END\" indicating that the angent should end and return the current response\n",
        "5. If the LLM deems it unhelpful, return \"CONTINUE\" indicating that the reasoning loop should continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### ADD CONDITIONAL EDGE AFTER THE AGENT SPECIFYING WHAT NODE EACH OUTPUT MAPS TO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVTKnWMbP_8T",
        "outputId": "7f729b1f-311c-4084-ceaf-0da437900c85"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### ADDING AN EDGE FROM THE ACTION BACK TO THE AGENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbDK2MbuREgU",
        "outputId": "21a64c20-27a1-4e0e-afde-a639abaa8b55"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### COMPILING THE GRAPH SO IT CAN BE RAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### EXECUTING THE GRAPH AND PRINTING UPDATES AT EACH NODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "f152dea8-96ad-4d29-d8b2-a064c96a8bd3"
      },
      "outputs": [],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "d847426e-71b3-47e6-b1ae-351a78d68d1e"
      },
      "outputs": [],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
